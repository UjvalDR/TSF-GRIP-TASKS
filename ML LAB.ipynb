{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.FIND-S\n",
    "\n",
    "import csv\n",
    "with open('enjoysport.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader) \n",
    "print(\"Training data\\n\")\n",
    "for row in data:\n",
    "    print(row)\n",
    "\n",
    "attr_len = len(data[0])-1\n",
    "h = ['0']*attr_len\n",
    "print(\"h=\",h)\n",
    "k=0\n",
    "\n",
    "print(\"\\nThe Hypothesis are\\n\")\n",
    "for row in data:\n",
    "    \n",
    "    if row[-1] == 'yes':\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            if col != 'yes':\n",
    "                if col != h[j] and h[j] == '0':\n",
    "                    h[j] = col\n",
    "                elif col != h[j] and h[j] != '0':\n",
    "                    h[j] = '?'\n",
    "                        \n",
    "            j = j + 1\n",
    "    print(\"h\",k,\"=\",h) \n",
    "    k=k+1\n",
    "        \n",
    "print('\\nMaximally Specific Hypothesis: \\n',\"h\",k-1,\"=\", h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CANDIDATE_ELIMINATION\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('enjoysport.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    \n",
    "#Training data from CSV file\n",
    "print(\"Training data\\n\")\n",
    "for row in data:\n",
    "    print(row)\n",
    "print(\"--------------------------------------\")\n",
    "\n",
    "attr_len=len(data[0])-1\n",
    "\n",
    "#initialize Specific and General Hypothesis\n",
    "S = ['0']*attr_len\n",
    "G = ['?']*attr_len\n",
    "temp=[]            # altered G\n",
    "\n",
    "print(\"The Hypothesis are\\n\")\n",
    "print(\"S=\",S)\n",
    "print(\"G=\",G)\n",
    "print(\"--------------------------------------\")\n",
    "for row in data:\n",
    "    if row[-1] == 'yes':\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            if col != 'yes':\n",
    "                if col != S[j] and S[j] == '0':\n",
    "                    S[j] = col\n",
    "                elif col != S[j] and S[j] != '0':\n",
    "                    S[j] = '?'\n",
    "            j = j + 1\n",
    "        \n",
    "        for j in range(0,attr_len):\n",
    "            for k in temp:\n",
    "                if k[j] != S[j] and k[j] != '?':\n",
    "                    temp.remove(k)\n",
    "    if row[-1]=='no':\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            if col != 'no':\n",
    "                if col!= S[j] and S[j] != '?':\n",
    "                    G[j]=S[j]\n",
    "                    temp.append(G)\n",
    "                    G=['?']*attr_len\n",
    "            j =j + 1\n",
    "    print(\"S=\",S) \n",
    "    if len(temp)==0:\n",
    "        print(\"G=\",G)\n",
    "    else:\n",
    "        print(\"G=\",temp)\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 ID3\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('id3.csv')\n",
    "print(\"\\n Input Data Set is:\\n\", df)\n",
    "\n",
    "t = df.keys()[-1]\n",
    "print('Target Attribute is: ', t)\n",
    "attribute_names = list(df.keys())\n",
    "attribute_names.remove(t) \n",
    "print('Predicting Attributes: ', attribute_names)\n",
    "\n",
    "\n",
    "def entropy(probs): \n",
    "    return sum( [-prob*math.log(prob, 2) for prob in probs])\n",
    "\n",
    "\n",
    "def entropy_of_list(ls,value):  \n",
    "    from collections import Counter\n",
    "    cnt = Counter(x for x in ls)\n",
    "    print('Target attribute class count(Yes/No)=',dict(cnt))\n",
    "    total_instances = len(ls)  \n",
    "    print(\"Total no of instances/records associated with {0} is: {1}\".format(value,total_instances ))\n",
    "    probs = [x / total_instances for x in cnt.values()]\n",
    "    return entropy(probs) \n",
    "\n",
    "def information_gain(df, split_attribute, target_attribute,battr):\n",
    "    print(\"\\n\\n-----Information Gain Calculation of \",split_attribute, \" --------\") \n",
    "    df_split = df.groupby(split_attribute)\n",
    "    glist=[]\n",
    "    for gname,group in df_split:\n",
    "        print('Grouped Attribute Values \\n',group)\n",
    "        glist.append(gname) \n",
    "    nobs = len(df.index)    \n",
    "    df_agg1=df_split.agg({target_attribute:lambda x:entropy_of_list(x, glist.pop())})\n",
    "    df_agg2=df_split.agg({target_attribute :lambda x:len(x)/nobs})\n",
    "    df_agg1.columns=['Entropy']\n",
    "    df_agg2.columns=['Proportion']\n",
    "    new_entropy = sum( df_agg1['Entropy'] * df_agg2['Proportion'])\n",
    "    if battr !='S':\n",
    "        old_entropy = entropy_of_list(df[target_attribute],'S-'+df.iloc[0][df.columns.get_loc(battr)])\n",
    "    else:\n",
    "        old_entropy = entropy_of_list(df[target_attribute],battr)\n",
    "    return old_entropy - new_entropy\n",
    "\n",
    "\n",
    "def id3(df, target_attribute, attribute_names, default_class=None,default_attr='S'):\n",
    "    from collections import Counter\n",
    "    cnt = Counter(x for x in df[target_attribute])     # class of YES /NO\n",
    "    if len(cnt) == 1:\n",
    "        return next(iter(cnt))  \n",
    "    elif df.empty or (not attribute_names):\n",
    "        return default_class  \n",
    "    else:\n",
    "        default_class = max(cnt.keys()) \n",
    "        gainz=[]        \n",
    "        for attr in attribute_names:\n",
    "            ig= information_gain(df, attr, target_attribute,default_attr)\n",
    "            gainz.append(ig)\n",
    "            print('Information gain of ',attr,' is : ',ig)\n",
    "        index_of_max = gainz.index(max(gainz))              \n",
    "        best_attr = attribute_names[index_of_max]           \n",
    "        print(\"\\nAttribute with the maximum gain is: \", best_attr)\n",
    "        tree = {best_attr:{}} \n",
    "        remaining_attribute_names =[i for i in attribute_names if i != best_attr]        \n",
    "        for attr_val, data_subset in df.groupby(best_attr):\n",
    "            subtree = id3(data_subset,target_attribute, remaining_attribute_names,default_class,best_attr)\n",
    "            tree[best_attr][attr_val] = subtree\n",
    "        return tree\n",
    "\n",
    "    \n",
    "from pprint import pprint\n",
    "\n",
    "tree = id3(df,t,attribute_names)\n",
    "print(\"\\nThe Resultant Decision Tree is:\")\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ANN BY BACKPROPAGATION\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [86], [89]), dtype=float)\n",
    "X = X/np.amax(X,axis=0)                          #maximum of X array longitudinally\n",
    "y = y/100\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Variable initialization\n",
    "epoch=5          #Setting training iterations\n",
    "lr=0.1           #Setting learning rate\n",
    "\n",
    "inputlayer_neurons = 2         #number of features in data set\n",
    "hiddenlayer_neurons = 3        #number of hidden layers neurons\n",
    "output_neurons = 1             #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "#draws a random range of numbers uniformly of dim x*y\n",
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    hinp1=np.dot(X,wh)\n",
    "    hinp=hinp1 + bh\n",
    "    hlayer_act = sigmoid(hinp)\n",
    "    outinp1=np.dot(hlayer_act,wout)\n",
    "    outinp= outinp1+bout\n",
    "    output = sigmoid(outinp)  \n",
    "    \n",
    "    #Backpropagation\n",
    "    EO = y-output\n",
    "    outgrad = derivatives_sigmoid(output)\n",
    "    d_output = EO * outgrad\n",
    "    EH = d_output.dot(wout.T)\n",
    "    hiddengrad = derivatives_sigmoid(hlayer_act)    #how much hidden layer wts contributed to error\n",
    "    d_hiddenlayer = EH * hiddengrad\n",
    "    \n",
    "    wout += hlayer_act.T.dot(d_output) *lr         # dotproduct of nextlayererror and currentlayerop\n",
    "    bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "    wh += X.T.dot(d_hiddenlayer) *lr\n",
    "    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "    \n",
    "print(\"Input: \\n\\n\" ,X) \n",
    "print(\"-\"*100)\n",
    "print(\"Actual Output: \\n\\n\" ,y)\n",
    "print(\"-\"*100)\n",
    "print(\"Predicted Output: \\n\\n\" ,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Naive Bayes\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename,\"r\") as csvfile:\n",
    "        datareader=csv.reader(csvfile)\n",
    "        traindata=list(datareader)\n",
    "    metadata=traindata[0]    #attributes name\n",
    "    traindata=traindata[1:]  #training examples      \n",
    "    return (metadata, traindata)\n",
    "\n",
    "def splitdataset(dataset, splitratio):\n",
    "    trainsize=int(len(dataset)*splitratio)\n",
    "    trainset=[]\n",
    "    testset=list(dataset)\n",
    "    i=0\n",
    "    while len(trainset)<trainsize:\n",
    "        trainset.append(testset.pop(i))\n",
    "    return [trainset,testset]\n",
    "    \n",
    "def classify(data,test):\n",
    "    totalsize=data.shape[0]\n",
    "    print(\"\\n\")\n",
    "    print(\"training data size=\",totalsize)\n",
    "    print(\"test data size=\",test.shape[0])\n",
    "    \n",
    "    countyes=0\n",
    "    countno=0\n",
    "    probyes=0\n",
    "    probno=0\n",
    "    print(\"\\n\")\n",
    "    print(\"target \\t count \\t probolaity\")\n",
    "    for x in range(data.shape[0]):\n",
    "        if data[x,data.shape[1]-1] =='yes':\n",
    "            countyes+=1\n",
    "        if data[x,data.shape[1]-1] =='no':\n",
    "            countno+=1\n",
    "    probYes=countyes/totalsize\n",
    "    probNo=countno/totalsize\n",
    "    \n",
    "    print(\"Yes \\t\", countyes,\"\\t\", probYes)\n",
    "    print(\"No \\t\", countno,\"\\t\", probNo)\n",
    "    \n",
    "    prob0=np.zeros((test.shape[1]-1))\n",
    "    prob1=np.zeros((test.shape[1]-1))\n",
    "    print(\"\\n prob0=\",prob0)\n",
    "    \n",
    "    accuracy=0\n",
    "    print(\"\\n\")\n",
    "    print(\"instance \\t prediction \\t target\")\n",
    "    \n",
    "    for t in range(test.shape[0]):\n",
    "        for k in range (test.shape[1]-1):\n",
    "            count1=count0=0\n",
    "            for j in range(data.shape[0]):\n",
    "                #how many times appeared with no\n",
    "                if test[t,k]==data[j,k] and data[j,data.shape[1]-1]=='no':\n",
    "                    count0+=1\n",
    "                #how many times appeared with yes\n",
    "                if test[t,k]==data[j,k] and data[j,data.shape[1]-1]=='yes':\n",
    "                    count1+=1  \n",
    "            prob0[k]=count0/countno\n",
    "            prob1[k]=count1/countyes\n",
    "        probno=probNo\n",
    "        probyes=probYes\n",
    "        for i in range(test.shape[1]-1):\n",
    "            probno=probno*prob0[i]\n",
    "            probyes=probyes*prob1[i]\n",
    "        if probno>probyes:\n",
    "            predict='no'\n",
    "        else:\n",
    "            predict='yes'\n",
    "        \n",
    "        print(\"  \",t+1,\" \\t\\t\",predict,\" \\t\\t\",test[t,test.shape[1]-1])\n",
    "        if predict==test[t,test.shape[1]-1]:\n",
    "            accuracy+=1\n",
    "    finalaccuracy=(accuracy/test.shape[0])*100\n",
    "    print(\"\\n Accuracy=\",finalaccuracy,\"%\")\n",
    "            \n",
    "        \n",
    "        \n",
    "metadata,traindata=read_data(\"id3.csv\")\n",
    "print(\"\\n The attribute names of training data are:\",metadata)\n",
    "splitratio=0.7\n",
    "trainset, testset=splitdataset(traindata, splitratio)\n",
    "training=np.array(trainset)\n",
    "\n",
    "print(\"\\n The Training data set are:\")\n",
    "for x in training:\n",
    "    print(x)\n",
    "\n",
    "testing=np.array(testset)\n",
    "print(\"\\n The Test data set are:\")\n",
    "for x in testing:\n",
    "    print(x)\n",
    "    \n",
    "classify(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Naive Bayes Classifier\n",
    "\n",
    "import pandas as pd \n",
    "msg=pd.read_csv('data6.csv',names=['message','label']) #Tabular form data \n",
    "print('Total instances in the dataset:',msg.shape[0])\n",
    "\n",
    "msg['labelnum']=msg.label.map({'pos':1,'neg':0}) \n",
    "#print(msg)\n",
    "X=msg.message\n",
    "Y=msg.labelnum\n",
    "\n",
    "#print(X)\n",
    "#print(Y)\n",
    "\n",
    "\n",
    "# Splitting the dataset into train and test data\n",
    "from sklearn.model_selection import train_test_split \n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,Y) \n",
    "\n",
    "print('\\nDataset is split into Training and Testing samples') \n",
    "print('Total training instances :', ytrain.shape[0]) \n",
    "print('Total testing instances :', ytest.shape[0])\n",
    "\n",
    "# Output of count vectoriser is a sparse matrix\n",
    "# CountVectorizer - stands for 'feature extraction'\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "count_vect = CountVectorizer()\n",
    "xtrain_dtm = count_vect.fit_transform(xtrain) #Sparse matrix \n",
    "xtest_dtm = count_vect.transform(xtest)\n",
    "print('\\nTotal features extracted using CountVectorizer:',xtrain_dtm.shape[1])\n",
    "\n",
    "print('\\nThe words or Tokens in the text documents\\n') \n",
    "print(count_vect.get_feature_names())\n",
    "\n",
    "\n",
    "# Training Naive Bayes (NB) classifier on training data. \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(xtrain_dtm,ytrain) \n",
    "predicted = clf.predict(xtest_dtm)\n",
    "\n",
    "#printing accuracy metrics \n",
    "from sklearn import metrics \n",
    "print('\\nAccuracy metrics')\n",
    "print('==================')\n",
    "print('Accuracy of the classifer is',metrics.accuracy_score(ytest,predicted))\n",
    "\n",
    "print('Recall :',metrics.recall_score(ytest,predicted), '\\nPrecison :',metrics.precision_score(ytest,predicted))\n",
    "print('Confusion matrix')\n",
    "print('==================')\n",
    "print(metrics.confusion_matrix(ytest,predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Bayesian Model for Heatdisease\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#read Cleveland Heart Disease data\n",
    "data = pd.read_csv('heart.csv')\n",
    "data = data.replace('?',np.nan)\n",
    "#display the data\n",
    "print('Sample instances from the dataset are given below')\n",
    "print(data.head())\n",
    "#display the Attributes names and datatyes\n",
    "print('\\n Attributes and datatypes')\n",
    "print(data.dtypes)\n",
    "\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "#Creat Model- Bayesian Network\n",
    "#Defining the model structure. We can define the network by just -\n",
    "#passing a list of edges.\n",
    "model =BayesianModel([('age','heartdisease'),('sex','heartdisease'),\n",
    "                      ('exang','heartdisease'),('cp','heartdisease'),\n",
    "                      ('heartdisease','restecg'),('heartdisease','chol')])\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "nx.draw(model, with_labels = True); \n",
    "plt.show()\n",
    "\n",
    "#Learning CPDs using Maximum Likelihood Estimators for all the variables\n",
    "print('\\n Learning CPD using Maximum likelihood estimators')\n",
    "model.fit(data,estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "#print(model.get_cpds('cp'))\n",
    "\n",
    "# Inferencing with Bayesian Network\n",
    "print('\\n Inferencing with Bayesian Network:')\n",
    "infer = VariableElimination(model)\n",
    "\n",
    "#computing the Probability of HeartDisease given restecg\n",
    "print('\\n 1.Probability of HeartDisease given evidence=restecg :1')\n",
    "q1=infer.query(variables=['heartdisease'],evidence={'restecg':1})\n",
    "print(q1)\n",
    "\n",
    "#computing the Probability of HeartDisease given cp\n",
    "print('\\n 2.Probability of HeartDisease given evidence= cp:2 ')\n",
    "q2=infer.query(variables=['heartdisease'],evidence={'cp':2})\n",
    "print(q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 KMeans Clustering\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import sklearn.metrics as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\n",
    "\n",
    "y = pd.DataFrame(iris.target)\n",
    "y.columns = ['Targets']\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "colormap = np.array(['red', 'lime', 'black'])\n",
    "\n",
    "# Plot the Original Classifications\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y.Targets], s=40)\n",
    "plt.title('Real Classification')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "\n",
    "\n",
    "# Plot the Models Classifications\n",
    "#KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(X)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[model.labels_], s=40)\n",
    "plt.title('K Mean Classification')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "print('The accuracy score of K-Mean: ',sm.accuracy_score(y, model.labels_))\n",
    "print('The Confusion matrixof K-Mean: ',sm.confusion_matrix(y, model.labels_))\n",
    "\n",
    "#EM(GMM)\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(X)\n",
    "y_gmm = gmm.predict(X)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y_gmm], s=40)\n",
    "plt.title('GMM Classification')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "\n",
    "print('The accuracy score of EM: ',sm.accuracy_score(y, y_gmm))\n",
    "print('The Confusion matrix of EM: ',sm.confusion_matrix(y, y_gmm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
